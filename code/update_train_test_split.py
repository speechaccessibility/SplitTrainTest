'''
Read in JSON files from a Speech Accessibility Project corpus, and generate a train/dev/test split.
The train/dev/test split is generated by the following rules:
  (1) all data from a contributor goes in the same split
  (2) if a contributor has been previously assigned to a split, they stay there
  (3) fraction in each split are 80%, 5%, 15% rounded to integers
  (4) subject to those constraints, contributors are assigned in a manner that minimizes
  the sum over pairs ((train,dev),(train,test),(test,dev)), of the sum over ratings,
  of the L1 Wasserstein distance between rating distributions in the two splits.

Written: Mark Hasegawa-Johnson
Date: 2023 July 17
License: MIT License
'''

import argparse, json, copy, os.path, glob, collections, random
import numpy as np

TRAIN_DEV_TEST_FRACTIONS = np.array([ 0.80, 0.05, 0.15])

# Assumed structure of dataset:
# datadir/contributor_id/contributor_id.json contains
#
# "Contributor ID": str
# "Files": [
#   {
#      "Ratings": [
#        {
#         "Level" : str of an int
#         "Dimension Description" : str
#        }
#      ]
#   }
# ]
#

def cache_key(assignment):
    '''
    Convert an assignment into a tuple of tuples, for use as a key to the cache
    @return:
    tuple of tuples: ((train contributor_ids),(dev contributor_ids),(test contributor_ids))
    '''
    return (tuple(sorted(assignment['train'])),
            tuple(sorted(assignment['dev'])),
            tuple(sorted(assignment['test'])))
    

class Score_Cache():
    def __init__(self, feature, num_new, prev, unassigned):
        '''
        A score cache stores scores calculated for previous assignments.
        Each new call of score_assignment checks the cache first,
        return new value only if the cache doesn't already have the requested assignment.
        @param:
        feature (dict): feature[feature_name][contributor_id] is a list of integers
        num_new (dict): num_new['train'] is the number of new train contributors to assign, 'dev','test'
        prev (dict): prev['train'] is the list of previously assigned train contributors, 'dev','test'
        unassigned (list): a list of the contributor IDs in feature but not in prev
        '''
        self.feature = {f:{c:[x for x in feature[f][c]] for c in feature[f]} for f in feature}
        self.num = {s:n for (s,n) in num_new.items()}
        self.assigned = tuple(tuple(c for c in prev[s]) for s in ['train','dev','test'])
        self.unassigned = [c for c in unassigned]
        self.cache = {}

    def randomly_assign(self):
        '''
        Generate a random assignment matching known target number and previous split.
        @return:
        assignment (tuple of tuples): assignment[n]==tuple of contributor_ids, n in 0,1,2
        '''
        assignment = [ list(split) for split in self.assigned ]
        random.shuffle(self.unassigned)
        assignment[0].extend(self.unassigned[:self.num['train']])
        assignment[1].extend(self.unassigned[self.num['train']:self.num['train']+self.num['dev']])
        assignment[2].extend(self.unassigned[self.num['train']+self.num['dev']:])
        #print(self.num, assignment)
        #print('randomly_assign:',assignment['train'][0],assignment['dev'][0],assignment['test'][0])
        return (tuple(sorted(assignment[0])),
                tuple(sorted(assignment[1])),
                tuple(sorted(assignment[2])))

    def score_assignment(self, assignment):
        '''
        Compute score for a given assignment.
        Score = sum over pairs (train,dev), (train,test), (test,dev)
          of sum over feature dimensions
          of the L1 Wasserstein distance between feature lists of the two pairs.
          L1 Wasserstein is the expected absolute feature value shift necessary to make
          one distribution of feature values equal the other.

        for each feature dimension:
         train = sorted concatenation of values from train speakers (O(nlogn))
         dev = sorted concatenation of values from dev speakers (O(nlogn))
         test = sorted concatenation of values from test speakers (O(nlogn))
         score = sum of absolute differences, scaled by 1/len(longer of the two lists)
        
        @param:
        assignment (tuple): assignment[0,1,2] are sorted train, dev, test tuples of contributor IDs
        self.feature[feature][contributor] = list of numbers, can use += to concatenate
        '''
        if assignment in self.cache:
            return self.cache[assignment]
        score = 0
        for fkey,fval in self.feature.items():
            tr = np.array(sorted([ val for c in assignment[0] if c in fval for val in fval[c] ]))
            de = np.array(sorted([ val for c in assignment[1] if c in fval for val in fval[c] ]))
            te = np.array(sorted([ val for c in assignment[2] if c in fval for val in fval[c] ]))
            for (s0,s1) in [(tr,de),(tr,te),(te,de)]:
                if len(s1) > len(s0):  # Swap so s0 is always longer
                    s0,s1 = s1,s0
                if len(s1) == 0 and len(s0) > 0:  # if len(s1)==0, put its probability mass at 0
                    score += np.sum(s0)/len(s0)
                elif len(s0) > 0:             # prob. mass quantized in units of 1/len(s0)
                    i0 = np.arange(len(s0))
                    i1 = np.floor(i0*len(s1)/len(s0)).astype('int')
                    score += np.sum(np.abs(s0[i0]-s1[i1]))/len(s0)
        self.cache[cache_key] = score
        return score

    def print_differences(self, a1, a2):
        '''
        Print differences between two assignments.
        Differences = split:contributor_id that exists in a2 but not a1.
        '''
        delta = { s:[] for s in ['train','dev','test'] }
        for n,s in enumerate(['train','dev','test']):
            for c in a2[n]:
                if c not in a1[n]:
                    delta[s].append(c)
        return delta
    
    def coordinate_search(self, original):
        '''
        Find the best score reachable by coordinate descent from initial assignment
        @param:
        original (tuple): original[0,1,2] = tuple of train,dev,test contributor IDs
        @return:
        score (real): score of best assignment (sum of L1 Wasserstein distances)
        assignment (tuple): best assignment (tuple of tuples of contributor IDs)
        pathlength (int): number of steps that were taken from original assignment
        '''
        assignment = tuple(tuple(c for c in original[n]) for n in range(3))
        
        prev_score = np.inf
        score = self.score_assignment(assignment)
        pathlength = -1
        while score < prev_score:
            pathlength += 1
            prev_score = score
            for n, test_assignment in enumerate(neighbors(assignment)):
                test_score = self.score_assignment(test_assignment)
                if n % 1000 == 0:
                    delta = self.print_differences(assignment, test_assignment)
                    print('Pathlength %d, neighbor %d: %g vs %g:'%(pathlength,n,test_score,score),delta)
                if test_score < score:
                    score = test_score
                    assignment = test_assignment
        delta = self.print_differences(original, assignment)
        print('Coordinate search changed:',delta)
        
        return score, assignment, pathlength

    def create_output_dict(self, assignment, score):
        '''
        Create an output JSON object that includes the assignment, score, 
        and a listing of the features sorted into each assignment category
        so that the user can compare their similarity.
        
        @param:
        assignment (tuple) - assignment[0,1,2] are train,dev,test tuples of contributor IDs
        score (real) - sum across pairs of sum across features of the L1 Wasserstein distance.
        This should be a number between 0 and 3*N*5, where 3 is the number of pairs,
        N is the number of distinct features, and 5 is the maximum score in each feature.

        @return:
        outputdict (dict) -
        - entries 'train', 'dev', and 'test' are just like in 'assignment'
        - entry 'score' gives the score
        - outputdict[feature][split][level] is an integer specifying the number
          of distinct times that level was observed for that feature in that split.
        '''
        outputdict = {}
        for n, split in enumerate(['train','dev','test']):
            outputdict[split] = [ contributor for contributor in assignment[n] ]
        outputdict['score'] = score
        for f in self.feature:
            outputdict[f] = {'train':[], 'dev':[], 'test':[] }
            for split in ['train','dev','test']:
                for c in outputdict[split]:
                    if c in self.feature[f]:
                        for val in self.feature[f][c]:
                            while len(outputdict[f][split]) <= val:
                                outputdict[f][split].append(0)
                            outputdict[f][split][val] += 1
        return outputdict
                       


def neighbors(inp):
    '''
    Returns a generator that generates all neighboring assignments.
    'Neighboring' means that one pair of contributors has been swapped
    from train to dev, from dev to test, or from train to test,
    and the resulting tuples have been resorted.
    @param:
    inp (tuple) - inp[0,1,2] is tuple of train,dev,test contributor IDs
    @yield:
    ret (tuple) - ret[0,1,2] is tuple of train,dev,test contributor IDs
    '''
    for (s0,s1,s2) in [(0,1,2),(0,2,1),(2,1,0)]:
        for n0 in range(len(inp[s0])):
            for n1 in range(len(inp[s1])):
                r = {}
                r[s0] = tuple(sorted([ x for x in inp[s0] if x != inp[s0][n0] ]+[ inp[s1][n1] ]))
                r[s1] = tuple(sorted([ x for x in inp[s1] if x != inp[s1][n1] ]+[ inp[s0][n0] ]))
                r[s2] = inp[s2]
                yield (r[0],r[1],r[2])
        
def search_with_restarts(feature, num_new, prev, unassigned, beamwidth, n_restarts):
    '''
    Randomly restart the coordinate search <n_restarts> times.
    Return a list of the <beamdwidth> best distinct endpoints of those searches.

    @param:
    feature (dict) - feature[dimension_description][contributor_id] is an integer rating
    num_new (dict) - num_new[split] is the number of new contributors to assign to the split
    prev (dict) - prev['train'], prev['dev'], and prev['test'] are lists of contributor IDs
    unassigned (list) - a list of contributor IDs that exist in feature but not in prev
    beamwidth (int) - number of distinct search optima to store and return
    n_restarts (int) - number of times to restart the search

    @return:
    outputdicts (list) - list of dictionaries of the type created by 
      score_cache.create_output_dict.  If fewer than beamwidth distinct endpoints are returned,
      then len(outputdicts) will equal the number of distinct endpoints.
    '''
    best_scores = [np.inf]*beamwidth
    best_assignments = [None]*beamwidth
    score_cache = Score_Cache(feature, num_new, prev, unassigned)
    for i_restart in range(n_restarts):
        assignment = score_cache.randomly_assign()
        score, assignment, pathlength = score_cache.coordinate_search(assignment)
        for i_sc in range(len(best_scores)):
            if assignment == best_assignments[i_sc]:
                break # We already had this assignment - no need to store it again
            if score < best_scores[i_sc]:  # Otherwise, if it's better than i'th best, store it
                best_scores[i_sc], score = score, best_scores[i_sc]
                best_assignments[i_sc], assignment = assignment, best_assignments[i_sc]
        print('Restart %d: %g after %d iterations'%(i_restart,best_scores[0],pathlength))

    outputdicts = []
    for i_b in range(beamwidth):
        if best_assignments[i_b]:
            outputdicts.append(score_cache.create_output_dict(best_assignments[i_b],best_scores[i_b]))
    return outputdicts


def main(datadir, prevsplitfile, beamwidth, n_restarts, outputfile):
    '''
    Create and return a new train/dev/test split based on previous splits.
    @param:
    datadir (str): directory containing data
    prevsplit (str): file containing previous split, or none
    @return:
    outputsplit (dict): the output split
    '''

    # Read the ratings from speaker data files
    # feature[dimension_description][contributor_id] = list of integer scores
    jsonfiles = glob.glob(os.path.join(datadir, '*', '*.json'))
    feature = {}
    for jsonfile in jsonfiles:
        with open(jsonfile) as f:
            contributor = json.load(f)
            contributor_id = contributor['Contributor ID']
            for utterance in contributor['Files']:
                if 'Ratings' in utterance:
                    for rating in utterance['Ratings']:
                        dimension_description = rating['Dimension Description']
                        if dimension_description not in feature:
                            feature[dimension_description] = collections.defaultdict(list)
                        if rating['Level'].isnumeric():
                            feature[dimension_description][contributor_id].append(int(rating['Level']))
                            
    #print('%d dimension descriptions found:'%(len(feature)))
    #print({x:len(y) for (x,y) in feature.items()})

    # Prev split, if it exists, is a single JSON dict object
    if prevsplitfile == None:
        prev = {'train':[], 'dev':[], 'test':[]}
    else:
        with open(prevsplitfile,'r') as f:
            prev = json.load(f)

    # Create a list of not-yet-assigned contributors
    contributors = set(c for d in feature.values() for c in d.keys())
    #print('%d contributors found:'%(len(contributors)))
    #print(contributors)
    unassigned = list(contributors)
    for s in prev:
        for c in prev[s]:
            unassigned.delete(c)
    #print('%d were unassigned:'%(len(unassigned)))
    #print(unassigned)
            
    # Find ntrain, ndev, ntest as close as possible to target fractions
    target_real = TRAIN_DEV_TEST_FRACTIONS * len(contributors)
    #print(target_real)
    target_int = np.round(target_real).astype('int')
    #print(target_int)
    while np.sum(target_int) != len(contributors):
        error_sign = np.sign(np.sum(target_int)-len(jsonfiles))
        argmax_error = np.argmax(error_sign*(target_int - target_real))
        target_int[argmax_error] -= error_sign

    # Find newtrain, newdev, newtest as increments over previous split
    num_new = {
        'train': target_int[0] - len(prev['train']),
        'dev': target_int[1] - len(prev['dev']),
        'test': target_int[2] - len(prev['test'])
    }

    print('Will assign %d, %d, %d, for total of %d, %d, %d in train, dev, test'%(num_new['train'],num_new['dev'],num_new['test'],target_int[0],target_int[1],target_int[2]))
    
    outputdicts = search_with_restarts(feature, num_new, prev, unassigned, beamwidth, n_restarts)
    print('RESULT: %d restarts, beam=%d, %d distinct outputs'%(n_restarts,beamwidth,len(outputdicts)))

    (outputbase, outputext) = os.path.splitext(outputfile)
    for filenum in range(len(outputdicts)):
        with open("%s%d%s"%(outputbase,filenum,outputext), 'w') as f:
            json.dump(outputdicts[filenum], f, indent=2)
              
################################################################################################
# Command line arguments
#
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description     = 'Update the train/dev/test split to include new speakers.',
        formatter_class = argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('-p','--prevsplit',
                        help = 'Previous split')
    parser.add_argument('-d','--datadir',
                        help = 'Directory containing unsplit dataset')
    parser.add_argument('-b','--beamwidth', default='3',
                        help = 'Number of best splits to retur')
    parser.add_argument('-n','--n_restarts', default='100',
                        help = 'Number of times to randomly restart the coordinate descent')
    parser.add_argument('-o','--outputfile', default='newsplit.json',
                        help = '''
                        Output filenames are based on this name, with numbers for the nth best.
                        For example: newsplit0.json, newsplit1.json, newsplit2.json
                        will be JSON-encodings of the best, second, and third-best splits found.
                        ''')

    args   = parser.parse_args()
    if not args.datadir:
        raise RuntimeError('Please use -d to specify a datadir')
    if not args.prevsplit:
        print('You did not specify any previous split, so I will design a split from scratch')
        
    main(args.datadir, args.prevsplit, int(args.beamwidth),
         int(args.n_restarts), args.outputfile)
